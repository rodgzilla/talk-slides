\documentclass[9pt]{beamer}

\input{packages.tex}
\input{colors.tex}

\usetheme{Boadilla}
\title{Adversarial examples in deep learning}
\author{G. Châtel\\@rodgzilla\\github.com/rodgzilla}
\date{06/07/2017}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is an adversarial example?}

  An \emph{adversarial example} is a sample of input data which has
  been modified \emph{very slightly} in a way that is intended to
  cause a machine learning classifier to misclassify it.

  \bigskip

  \pause

  \begin{center}
    \includegraphics[trim={2pt 2pt 2pt 0}, clip, width =
      \linewidth]{images/adversarial_example_wig.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Basic concept}

  \begin{center}
    \scalebox{0.8}{
      \input{figures/parabola.tex}
    }
  \end{center}

  The curve needs to be \textit{smooth enough} for the gradient
  descent to work.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Model optimization}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/scatter_01.tex}
    }
  \end{center}

  We have a set of points that we want to approximate with a line.

  \[
  y = ax + b
  \]

  \pause

  First we choose a loss that measures how good our predictions are.

  \[
  l(x, y, a, b) = (y - (a x + b))^{2}
  \]

  \pause

  We compute how the loss is affected by small changes of $a$ and $b$.

  \[
  \frac{\mathrm{d}l}{\mathrm{d}a} = 2 x (ax + b - y) \qquad \qquad \frac{\mathrm{d}l}{\mathrm{d}b} = 2 (ax + b - y)
  \]

  And we update $a$ and $b$ iteratively until we reach a satisfying
  result (the average loss for our data points is low enough).

%  a = -0.08 \quad b = 0.68
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Being evil}

  \vspace{-0.5cm}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/scatter_02.tex}
    }
  \end{center}

  In our previous example, we have modified \textcolor{red}{the model}
  in order to minimize the loss.

  \[
  y = \textcolor{red}{a}x + \textcolor{red}{b}
  \]

  \pause

  Now suppose we are an attacker who wants to maximise the loss of a
  model, its \textcolor{red}{parameters} being fixed. The only thing
  we can modify is the \textcolor{blue}{inputs}.

  \[
  l(\textcolor{blue}{x}, y, a, b) = (y - (a \textcolor{blue}{x} + b))^{2}
  \]

  \pause

  In order to do this, we compute how the loss is affected by small
  changes of the input.

  \[
  \frac{\mathrm{d}l}{\mathrm{d}x} = 2 a (ax + b - y)
  \]

  We can now make \emph{imperceptible} changes to an input that will
  increase the loss value.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural networks}

  Everything works the same way when working with a neural network on
  an image classification task.

  \bigskip

  We also have a differentiable \textcolor{red}{loss function} (often
  \textcolor{red}{categorical cross entropy}) and
  \textcolor{blue}{inputs} (\textcolor{blue}{pixel values} in the case
  of images) that we can modify to increase the loss.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Attack}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Attacks}

%%   \begin{description}
%%   \item[Random noise] Does not work
%%   \item[FGSM] Good but can be well defended by training the network
%%     with adversarial samples
%%   \item[Iterative FGSM] Higher error than FGSM for an equivalent
%%     $\varepsilon$ but less transferability. I-FGSM produces weaker
%%     black-box attacks.
%%   \item[Targeted FGSM] Aims at fooling a model into outputting a given
%%     target class.
%%   \item[RAND + FGSM] Significant improvements against adversarially
%%     trained models. RAND+FGSM transfers at lower rates than FGSM
%%     examples. Unsing RAND+FGSM to adverarially train networks does not
%%     improve their defense against RAND+FGSM.
%%   \end{description}
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \begin{center}
    \includegraphics[width = 0.7 \linewidth]{images/wait-a-minute.jpg}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \framesubtitle{Nope.}

  \begin{center}
    \includegraphics[width = \linewidth]{images/random_perturbation.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method [Goodfellow et al. 2015]}

  Let $x$ be the original image, $\theta$ the parameters of the model,
  $y$ the target associated with $x$ and $J(\theta, x, y)$ the loss
  function.

  \bigskip

  We compute the gradient of the loss function according to the input
  pixels.

  \[
  \nabla_{x} J(\theta, x, y)
  \]

  \bigskip

  The perturbation is the signs of these derivatives multiplied by a
  small number $\varepsilon$.

  \[
  \eta = \varepsilon \text{sign}(\nabla_{x} J(\theta, x, y))
  \]

  \bigskip

  The final adversarial sample is the sum of the original image and
  the pertubation.

  \[
  x_{adv} = x + \eta
  \]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method}

  \framesubtitle{Using the \textbf{VGG16} network with imagenet
    weights}

  \begin{center}
    \input{figures/FGSM.tex}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black box attack [Papernot et al. 2016]}

  \framesubtitle{or good luck getting gradients out of your
    self-driving car}

  \begin{center}
    \includegraphics[width = 5cm]{images/cool_story_bro.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black box attack [Papernot et al. 2016]}

  \framesubtitle{Transferability of adversarial samples}

  %% If our interface with the target model is its final output, we
  %% cannot use gradients to create adversarial samples.

  We can train a new model $M'$ to solve the same classification task
  as the target model $M$.

  \pause

  \bigskip

  Once trained, we can create an adversarial sample $x_{adv}'$ for the
  $M'$ model and experiences have shown that $x_{adv}'$ will also fool
  $M$ very often.

  \pause

  \bigskip

  What if we do not have a training set for the target network?
  Well\dots{} build one using $M$ predictions.

  \pause

  \bigskip

  \textit{``After labeling 6,400 synthetic inputs to train our
    substitute (an order of magnitude smaller than the training set
    used by MetaMind) we find that their DNN misclassifies adversarial
    examples crafted with our substitute at a rate of 84.24\%''}

  \smallskip

  - Papernot et al., about their attack on the MetaMind deep neural
  network.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world [Kurakin et
      al. 2017]}

  \framesubtitle{Being evil, for real}

  In real world scenarios, the target network does not take our image
  files as input. It acquires the data by the network's system
  (\textit{e.g.} a camera).

  \pause

  \medskip

  It also works, for free.

  \begin{center}
    \includegraphics[width = 7cm]{images/physical_adversarial_sample.png}
  \end{center}

%  \bigskip

  \textit{``We used images taken from a cell-phone camera as a input
    to an Inception v3 image classification neural network. We showed
    that in such a set-up, a significant fraction of adversarial
    images crafted using the original network are misclassified even
    when fed to the classifier through the camera.''}

  \smallskip

  - Kurakin et al.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Defense}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Defenses}

  \begin{description}
  \item[Adversarial sample detection] We try to detect whether an
    input sample is adversarial or not before classifying it.
  \item[Regularization] Training with an adversarial objective
    function is an effective regularizer (from [explaining and
      harnessing]).
  \item[Gradient masking] The goal of gradient masking is to leave the
    decision boundaries untouched but damage the gradient used in
    white-box attacks.
  \item[Distillation and network saturation] These methods are used to
    introduce numerical instabilities in gradient computations.
  \end{description}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Defending machine learning}

  ``Most defenses against adversarial examples that have been proposed
  so far just do not work very well at all, but the ones that do work
  are not adaptive. This means it is like they are playing a game of
  whack-a-mole: they close some vulnerabilities, but leave others
  open.''

  \bigskip

  - Ian Goodfellow, Nicolas Papernot, February 2017
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{References}

  {\footnotesize
    \begin{enumerate}
    \item Goodfellow, I. J., Shlens, J., \& Szegedy,
      C. (2014). Explaining and harnessing adversarial
      examples. arXiv preprint arXiv:1412.6572.
    \item Papernot, N., McDaniel, P., Wu, X., Jha, S., \& Swami,
      A. (2016). Distillation as a defense to adversarial
      perturbations against deep neural networks. In Security and
      Privacy (SP), 2016 IEEE Symposium on (pp. 582-597). IEEE.
    \item Kurakin, A., Goodfellow, I., \& Bengio,
      S. (2016). Adversarial examples in the physical world. arXiv
      preprint arXiv:1607.02533.
    \item Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik,
      Z. B., \& Swami, A. (2016). Practical black-box attacks against
      deep learning systems using adversarial examples. arXiv preprint
      arXiv:1602.02697.
    \item Tramèr, F., Kurakin, A., Papernot, N., Boneh, D., \&
      McDaniel, P. (2017). Ensemble Adversarial Training: Attacks and
      Defenses. arXiv preprint arXiv:1705.07204.
    \end{enumerate}
  }

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
